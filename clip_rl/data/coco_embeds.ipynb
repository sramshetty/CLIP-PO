{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import open_clip\n",
    "\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch COCO Captions and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_file = \"../../LAION/Data/coco_captions/captions_train2017.json\"\n",
    "image_dir = \"../../LAION/Data/coco_train2017/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.87s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A bicycle replica with a clock as the front wheel.',\n",
       " 'A room with blue walls and a white sink and door.',\n",
       " 'A car that seems to be parked illegally behind a legally parked car',\n",
       " 'A large passenger airplane flying through the air.',\n",
       " 'There is a GOL plane taking off in a partly cloudy sky.',\n",
       " 'Blue and white color scheme in a small bathroom.',\n",
       " 'This is a blue and white bathroom with a wall sink and a lifesaver on the wall.',\n",
       " 'A blue boat themed bathroom with a life preserver on the wall',\n",
       " 'The bike has a clock as a tire.',\n",
       " 'two cars parked on the sidewalk on the street']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_captions = COCO(caption_file)\n",
    "captions = [cap_dict[\"caption\"] for cap_dict in coco_captions.anns.values()]\n",
    "captions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k')\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('ViT-H-14')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Caption Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encode_text(captions, batch_size=64):\n",
    "    feats = []\n",
    "    for i in tqdm(range(0, len(captions), batch_size)):\n",
    "        with torch.no_grad():\n",
    "            texts = tokenizer(captions[i:i+batch_size])\n",
    "            text_features = model.encode_text(texts.to(device))\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        feats.append(text_features.detach().cpu())\n",
    "    \n",
    "    feat_tensor = torch.cat(feats, dim=0)\n",
    "    np.save(\"./embeds/clip-H-14_text_coco_train17.npy\", feat_tensor.numpy(), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f8053895774f05b49761a4a81bdfdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_encode_text(captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(Image.open(\"C:\\\\Users\\\\Shivaen\\\\Documents\\\\Code\\\\LAION\\\\Data\\\\coco_train2017\\\\000000000072.jpg\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encode_img(image_dir, batch_size=64):\n",
    "    image_tensors = []\n",
    "    files = sorted(os.listdir(image_dir))\n",
    "\n",
    "    for i in tqdm(range(0, len(os.listdir(image_dir)), batch_size)):\n",
    "        images = []\n",
    "        for filename in files[i:i+batch_size]: \n",
    "            images.append(preprocess(Image.open(os.path.join(image_dir, filename))).unsqueeze(0))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_feats = model.encode_image(torch.cat(images, dim=0).to(device))\n",
    "            image_feats /= image_feats.norm(dim=-1, keepdim=True)\n",
    "            image_tensors.append(image_feats.detach().cpu())\n",
    "\n",
    "    np.save(\"./embeds/clip-H-14_img_coco_train17.npy\", torch.cat(image_tensors, dim=0).numpy(), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25f3100940e42e5bab0204371f26582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3697 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_encode_img(image_dir, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
